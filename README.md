### Util Functions
    - get_word_tag: Returns words and tags from the corpus. If some words are not in the vocabulary (which consists of HMM words), this function handles them.
    - preprocess: Checks for unknown words in the test corpus.
    - assign_unk: Handles unknown word tokens using morphology rules.
        
### Vocab
    - Stores words that appear more than twice and manages unknown tokens for rare words.
    - Unknown tokens (e.g., --unk-verb--, --unk-noun--) replace unknown words in both the training and test corpus, and are incorporated into emission, transition, and tag data structures.
    
### Dictionaries 
    - trans_counts: Records counts of transitions between tags.
    - emis_counts: Records counts of word occurrences given a tag.
    - tag_counts: Records counts of individual tags.

### Matrix
    - trans_matrix:
        Stores transition probabilities between tags, computed with smoothing
        The smoothing:
            ğ‘ƒ(ğ‘¡ğ‘–|ğ‘¡ğ‘–âˆ’1)=ğ¶(ğ‘¡ğ‘–âˆ’1,ğ‘¡ğ‘–)+ğ›¼ / ğ¶(ğ‘¡ğ‘–âˆ’1)+ğ›¼âˆ—ğ‘
        ğ›¼: alpha(constant value)
        ğ‘: num of tags
        ğ¶(ğ‘¡ğ‘–âˆ’1,ğ‘¡ğ‘–): trans_counts
        ğ¶(ğ‘¡ğ‘–âˆ’1): tag_counts
    
    - emis_matrix:
        Stores emission probabilities of words given tags, also computed with smoothing:
        Dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags.
        The smoothing:
            ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¡ğ‘–)=ğ¶(ğ‘¡ğ‘–,ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘–)+ğ›¼ / ğ¶(ğ‘¡ğ‘–)+ğ›¼âˆ—ğ‘
        ğ›¼: alpha(constant value)
        N: num of tags
        ğ¶(ğ‘¡ğ‘–,ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘–): emis_counts
        ğ¶(ğ‘¡ğ‘–): tag_counts
        
### Viterbi forward
    - best_probs:
        n x t matrix (n is num of unique POS tags and t is num of words in corpus)
        the best prob for the given current word's POS tag and the position.
    - best_path:
        n x t matrix (n is num of unique POS tags and t is num of words in corpus)
        the unique integer ID of the best prob
    - algorithm:
        Computes the probability of each possible tag for each word and tracks the best probabilities.

### Viterbi backward
    - pred: 
        It is an array(same size as corpus) that the proper pos tags will be entered.
    - z:
        It is an array that will be used for saving an index of the best prob thru best path.
    - algorithm:
         set the last element of z to argmax(best_probs[:,-1]) and go backwards thru best_paths to find the index of best prob.
         Then save proper pos tag into pred.
         
### citation:
    https://medium.com/analytics-vidhya/parts-of-speech-pos-and-viterbi-algorithm-3a5d54dfb346