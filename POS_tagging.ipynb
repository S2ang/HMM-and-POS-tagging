{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b1f02b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3d8e22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions for handling oov and getting tags & words\n",
    "# Punctuation characters\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Morphology rules used to assign unknown word tokens\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "\n",
    "def get_word_tag(line, vocab): \n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        if word not in vocab: \n",
    "            # Handle unknown words\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "    return None \n",
    "\n",
    "\n",
    "def preprocess(vocab, data_fp):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    \"\"\"\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    # Read data\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, word in enumerate(data_file):\n",
    "\n",
    "            # End of sentence\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word = \"--n--\"\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            # Handle unknown words\n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word = assign_unk(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "\n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "\n",
    "    return orig, prep\n",
    "\n",
    "\n",
    "def assign_unk(tok):\n",
    "    \"\"\"\n",
    "    Assign unknown word tokens\n",
    "    \"\"\"\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5e4e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the training corpus\n",
    "with open(\"WSJ_POS_CORPUS_FOR_STUDENTS/WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a12495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making hmm vocab\n",
    "# words used more than twice are included \n",
    "# unknown tokens for handling unknown words\n",
    "vocab = {}\n",
    "words = []\n",
    "two_or_more = []\n",
    "\n",
    "for i in training_corpus:\n",
    "    items = i.split()\n",
    "    if not len(items) is 2 :\n",
    "        continue\n",
    "    words.append(items[0])\n",
    "count_words = Counter(words)\n",
    "\n",
    "for i in count_words.keys():\n",
    "    if count_words[i] >= 2:\n",
    "        two_or_more.append(i)\n",
    "\n",
    "un_token = ['--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--']\n",
    "new = [i for i in (Counter(un_token+two_or_more)).keys()]\n",
    "\n",
    "for i, word in list(enumerate(sorted(new))):\n",
    "    vocab[word] = i     \n",
    "\n",
    "orig, prep = preprocess(vocab, 'WSJ_POS_CORPUS_FOR_STUDENTS/WSJ_23.words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "985f47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    # initialize\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # default\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # get words and tags from training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        # helper function (imported from utils_pos.py)\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        # Increment the tag count\n",
    "        tag_counts[(tag)] += 1\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68f43f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c3a6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80ddc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    \n",
    "    # POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # num of POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix\n",
    "    trans_matrix = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    # row of the transition matrix \n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # column of the transition matrix \n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "        \n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in trans_keys:\n",
    "                \n",
    "                # Get count for the (prev POS, current POS) tuple\n",
    "                count = transition_counts.get(key)\n",
    "                \n",
    "            # Get the count of the previous \n",
    "            count_prev_tag = tag_counts.get(all_tags[i])\n",
    "            \n",
    "            # Apply smoothing \n",
    "            trans_matrix[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "    \n",
    "    return trans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7701e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A subset of transition matrix\n",
      "            NN       NNP      NNPS       NNS           PDT\n",
      "NN    0.122172  0.009749  0.000090  0.077797  1.505246e-05\n",
      "NNP   0.058328  0.376807  0.016695  0.024249  1.094395e-05\n",
      "NNPS  0.038159  0.277212  0.015713  0.011224  3.741050e-07\n",
      "NNS   0.020817  0.003057  0.000033  0.010525  5.013696e-05\n",
      "PDT   0.000003  0.000003  0.000003  0.000003  2.702367e-06\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "trans_matrix = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "\n",
    "print(\"A subset of transition matrix\")\n",
    "trans_matrix_sub = pd.DataFrame(trans_matrix[20:25,20:25], index=states[20:25], columns = states[20:25] )\n",
    "print(trans_matrix_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fa58f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    \n",
    "    # num of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # num of unique words in the vocab\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix \n",
    "    emis_matrix = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    # row (POS tags)\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        # column (words)\n",
    "        for j in range(num_words): # complete this line\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0\n",
    "                    \n",
    "            # (POS tag, word) tuple \n",
    "            key = (all_tags[i], vocab[j])\n",
    "\n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emis_keys: \n",
    "        \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts.get(key)\n",
    "                \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts.get(all_tags[i])\n",
    "                \n",
    "            # Apply smoothing\n",
    "            emis_matrix[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "\n",
    "    return emis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f6e475cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1987      computer     engineers          paid         level\n",
      "CD   8.201296e-05  2.732855e-08  2.732855e-08  2.732855e-08  2.732855e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723319e-07  3.723319e-07  3.723319e-07  3.723319e-07  3.723319e-07\n"
     ]
    }
   ],
   "source": [
    "emis_matrix = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "idx  = ['1987','computer','engineers', 'paid', 'level']\n",
    "\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "vals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "emis_matrix_sub = pd.DataFrame(emis_matrix[np.ix_(rows,cols)], index=vals, columns = idx )\n",
    "print(emis_matrix_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45badce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, trans_matrix, emis_matrix, corpus, vocab):\n",
    "    \n",
    "    # num of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # The start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "   \n",
    "    #  The POS tags\n",
    "    for i in range(num_tags): # complete this line\n",
    "    \n",
    "        # The special case when the transition from start token to POS tag i is zero\n",
    "        if trans_matrix[i][0] == 0: # complete this line\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
    "            best_probs[i,0] = float('-inf')\n",
    "        \n",
    "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
    "        else:\n",
    "            # Initialize best_probs at POS tag 'i', column 0\n",
    "            best_probs[i,0] = math.log(trans_matrix[s_idx, i]) + math.log(emis_matrix[i, vocab[corpus[0]]])\n",
    "                         \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "374b5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,5]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, trans_matrix, emis_matrix, prep, vocab)\n",
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,5]: {best_paths[2,5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c8fa8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(trans_matrix, emis_matrix, test_corpus, best_probs, best_paths, vocab):\n",
    "\n",
    "    # num of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    for i in range(1, len(test_corpus)): \n",
    "            \n",
    "        for j in range(num_tags): \n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "            \n",
    "            # list for storing probs of viterbi\n",
    "            probs = []\n",
    "            \n",
    "            for k in range(num_tags): \n",
    "            \n",
    "                # The prob = \n",
    "                # best probs of POS tag k, previous word i-1 + \n",
    "                # log(prob of transition POS k -> POS j) + \n",
    "                # log(prob that emission POS j -> word i)\n",
    "                prob = best_probs[k, i-1] + math.log(trans_matrix[k,j]) + math.log(emis_matrix[j,vocab[test_corpus[i]]])\n",
    "                \n",
    "                # stack every prob\n",
    "                probs.append(prob)\n",
    "            \n",
    "            # find max prob\n",
    "            best_prob_i = max(probs)\n",
    "            \n",
    "            # find index of max prob\n",
    "            best_path_i = probs.index(best_prob_i)\n",
    "            \n",
    "            # Save the best prob for the \n",
    "            # given current word's POS tag and the position \n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cab79e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute \n",
    "best_probs, best_paths = viterbi_forward(trans_matrix, emis_matrix, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "692e4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    \n",
    "    # num of words in the corpus\n",
    "    num_words = best_paths.shape[1] # col in best_paths\n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * num_words\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * num_words\n",
    "    \n",
    "    # Save the index of the best prob of the last col values \n",
    "    z[num_words-1] = np.argmax(best_probs[:,-1])\n",
    "    \n",
    "    # Initialize the last data \n",
    "    pred[num_words - 1] = states[z[num_words-1]]\n",
    "    \n",
    "    # Find the best POS tags by backtracking through the best_paths\n",
    "    for i in range(num_words-1, 0, -1): # complete this line\n",
    "        \n",
    "        # Retrieve the unique integer ID (the word at position 'i' in the corpus)\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        \n",
    "        # go backward thru best_paths and find the index of best prob\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        pred[i - 1] = states[z[i-1]]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "859b0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = viterbi_backward(best_probs, best_paths, prep, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a63d9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a file with words and proper tags \n",
    "with open('WSJ_POS_CORPUS_FOR_STUDENTS/submission.pos', 'w') as f:\n",
    "    for i, t in zip(orig, pred):\n",
    "        if t == \"--s--\":\n",
    "            f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(f\"{i}\\t{t}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529dcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
